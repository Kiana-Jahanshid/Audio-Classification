{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "JdlGeLTHCQiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bYBj5cT7H3HS",
        "outputId": "7d408a4e-cc5e-4ad1-e812-94caea11439b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install ffmpeg\n",
        "!pip install -q spleeter\n",
        "!pip install -q TeleBot\n",
        "!pip install -q pyTelegramBotAPI\n",
        "!pip install -q telegram\n",
        "!pip install -q pydub\n",
        "!pip install tensorflow==2.15.0"
      ],
      "metadata": {
        "id": "tJavYEk1e_vE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e42d289-7887-4363-dee7-c9cfc322b039"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "flask 2.2.5 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "dask 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "distributed 2023.8.1 requires click>=8.0, but you have click 7.1.2 which is incompatible.\n",
            "fiona 1.9.5 requires click~=8.0, but you have click 7.1.2 which is incompatible.\n",
            "pandas-gbq 0.19.2 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "pip-tools 6.13.0 requires click>=8, but you have click 7.1.2 which is incompatible.\n",
            "tensorflow-datasets 4.9.4 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyTelegramBotAPI (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for telegram (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Collecting flatbuffers>=23.5.26 (from tensorflow==2.15.0)\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (23.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.60.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.17.3)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.5.1)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow==2.15.0)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Installing collected packages: flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.3\n",
            "    Uninstalling tensorflow-2.9.3:\n",
            "      Successfully uninstalled tensorflow-2.9.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spleeter 2.4.0 requires tensorflow<2.10.0,>=2.5.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-23.5.26 google-auth-oauthlib-1.2.0 keras-2.15.0 protobuf-4.23.4 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-estimator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtE_dNckJmT5",
        "outputId": "98f50761-d5b6-4fbf-8266-c6a553472a2c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-estimator in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-io-gcs-filesystem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2poa-F0KAOK",
        "outputId": "4d0953b3-a0a1-4614-a503-a21fbdc2f455"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-io-gcs-filesystem in /usr/local/lib/python3.10/dist-packages (0.34.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BrIVv8nrxWyQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import telebot\n",
        "import telegram\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pydub\n",
        "from IPython.display import Audio\n",
        "from spleeter.separator import Separator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lfilYE7wxWyR"
      },
      "outputs": [],
      "source": [
        "#bot = telebot.TeleBot(\"6370453544:AAEI0zlqAp9HaPkuQv3o448Qs5YoYN1qSbs\")\n",
        "#model = tf.keras.models.load_model(\"audio_classification_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV4Av9WUxWyR",
        "outputId": "34db9de7-c631-4576-e608-0a3124cf9dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 files belonging to 1 classes.\n",
            "1/1 [==============================] - 0s 239ms/step\n",
            "5\n",
            "kiana\n",
            "The owner of this voice is : kiana\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-12-27 10:32:02,595 (__init__.py:968 MainThread) ERROR - TeleBot: \"Infinity polling: polling exited\"\n",
            "ERROR:TeleBot:Infinity polling: polling exited\n",
            "2023-12-27 10:32:02,599 (__init__.py:970 MainThread) ERROR - TeleBot: \"Break infinity polling\"\n",
            "ERROR:TeleBot:Break infinity polling\n"
          ]
        }
      ],
      "source": [
        "bot = telebot.TeleBot(\"6370453544:AAEI0zlqAp9HaPkuQv3o448Qs5YoYN1qSbs\")\n",
        "model = tf.keras.models.load_model(\"audio_classification_model.h5\")\n",
        "model2 = tf.keras.models.load_model(\"singers_weights.h5\")\n",
        "\n",
        "@bot.message_handler(commands=[\"start\"])\n",
        "def send_welcome(message):\n",
        "    bot.send_message(message.chat.id , \"Hello \" + message.from_user.first_name + \"\\nWelcome to Audio Recognizer bot🎙🎵🔊\" )\n",
        "    help(message)\n",
        "\n",
        "@bot.message_handler(commands=['help'])\n",
        "def help(message):\n",
        "    my_keyboard= telebot.types.ReplyKeyboardMarkup(row_width=3)\n",
        "    key1 = telebot.types.KeyboardButton(\"/Friends_voice_recognition\")\n",
        "    key2 = telebot.types.KeyboardButton(\"/Singers_voice_recognition\")\n",
        "    my_keyboard.row(key1)\n",
        "    my_keyboard.row(key2)\n",
        "    bot.send_message(message.chat.id ,\"Choose one of the below keys :\" , reply_markup=my_keyboard)\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['Friends_voice_recognition'])\n",
        "def mymessage(message):\n",
        "  message1 = bot.send_message(message.chat.id , \"Now send a voice ... \")\n",
        "  bot.register_next_step_handler(message1 , friends_voice_recognizer)\n",
        "\n",
        "@bot.message_handler(content_types=['voice'])\n",
        "def friends_voice_recognizer(message):\n",
        "  file_id = message.voice.file_id\n",
        "  new_file = bot.get_file(file_id)\n",
        "  file_path = bot.get_file(file_id).file_path\n",
        "  downloaded_file = bot.download_file(file_path)\n",
        "  try:\n",
        "      with open(\"/content/data/voice.ogg\", 'wb') as new_file:\n",
        "          new_file.write(downloaded_file)\n",
        "  except Exception as e:\n",
        "      bot.reply_to(message, \"Error saving file.\")\n",
        "      return\n",
        "\n",
        "  audio = pydub.AudioSegment.from_file(\"/content/data/voice.ogg\")\n",
        "  audio_array = audio.get_array_of_samples()\n",
        "  silent_chunks = pydub.silence.split_on_silence(audio , min_silence_len=2000 , silence_thresh=-45)\n",
        "  result = sum(silent_chunks)\n",
        "  result.export(\"/content/wav_data/voice.wav\")\n",
        "  audio = pydub.AudioSegment.from_file(\"/content/wav_data/voice.wav\")\n",
        "  chunks = pydub.utils.make_chunks(audio , 1000)\n",
        "  for i , chunk in enumerate(chunks) :\n",
        "      if len(chunk) >= 1000 :\n",
        "          chunk.export(os.path.join(\"/content/dataset/z\" , \"fvoice.wav\") , format=\"wav\")\n",
        "\n",
        "\n",
        "  test_data = tf.keras.utils.audio_dataset_from_directory(\n",
        "    \"/content/dataset\",\n",
        "    shuffle=False,batch_size=2,output_sequence_length= 48000,ragged= False,\n",
        "    labels=\"inferred\",label_mode=\"categorical\",sampling_rate= None,seed=59\n",
        "  )\n",
        "\n",
        "  classes_labels = np.array(['abdollah', 'azra', 'davood', 'javad', 'khadijeh', 'kiana','maryam', 'matin','melika', 'mohammad',\n",
        "                             'mohammad_parvari','mona', 'nahid', 'nima', 'omid', 'parisa','parsa', 'sadegh', 'sajedeh', 'shima', 'tara'])\n",
        "  predictions = model.predict(test_data)\n",
        "  probability = predictions\n",
        "  predicted_class_index = np.argmax(probability)\n",
        "  print(predicted_class_index)\n",
        "  predicted_class = classes_labels[predicted_class_index]\n",
        "  print(predicted_class)\n",
        "  print(f'The owner of this voice is : {predicted_class}')\n",
        "  bot.send_message(chat_id= message.chat.id , text=f'The owner of this voice is  :  {predicted_class}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#~~~~~~~~~~~~~~~~ singers audio recognition ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "@bot.message_handler(commands=['Singers_voice_recognition'])\n",
        "def mymessage2(message):\n",
        "  message2 = bot.send_message(message.chat.id , \"Now send a song...\")\n",
        "  bot.register_next_step_handler(message2 , singers_voice_recognition)\n",
        "\n",
        "@bot.message_handler(content_types=['audio'])\n",
        "def singers_voice_recognition(message):\n",
        "\n",
        "  file_id = message.audio.file_id\n",
        "  new_file = bot.get_file(file_id)\n",
        "  file_path = bot.get_file(file_id).file_path\n",
        "  downloaded_file = bot.download_file(file_path)\n",
        "  try:\n",
        "      with open(\"/content/data/audio.mp3\", 'wb') as new_file:\n",
        "          new_file.write(downloaded_file)\n",
        "  except Exception as e:\n",
        "      bot.reply_to(message, \"Error saving file.\")\n",
        "      return\n",
        "\n",
        "\n",
        "  audio_path = \"/content/data/audio.mp3\"\n",
        "  separator = Separator(\"spleeter:2stems\")\n",
        "  prediction = separator.separate_to_file(audio_path, \"/content/singers_vocals\")\n",
        "\n",
        "\n",
        "  audio = pydub.AudioSegment.from_file(\"/content/singers_vocals/audio/vocals.wav\")\n",
        "  audio_array = audio.get_array_of_samples()\n",
        "  silent_chunks = pydub.silence.split_on_silence(audio , min_silence_len=100 , silence_thresh=-50)\n",
        "  result = sum(silent_chunks)\n",
        "  result.export(\"/content/wav_data/audio.wav\")\n",
        "  audio = pydub.AudioSegment.from_file(\"/content/wav_data/audio.wav\")\n",
        "  chunks = pydub.utils.make_chunks(audio , 1000)\n",
        "  for i , chunk in enumerate(chunks) :\n",
        "      if len(chunk) >= 1000 :\n",
        "          chunk.export(os.path.join(\"/content/dataset_2/x\" , \"final_audio.wav\") , format=\"wav\")\n",
        "\n",
        "\n",
        "  test_data = tf.keras.utils.audio_dataset_from_directory(\n",
        "    \"/content/dataset_2\",\n",
        "    shuffle=False,batch_size=2,output_sequence_length= 48000,ragged= False,\n",
        "    labels=\"inferred\",label_mode=\"categorical\",sampling_rate= None,seed=59\n",
        "  )\n",
        "\n",
        "  classes_labels = np.array(['chaartar', 'chavoshi', 'sirvan', 'xaniar', 'yegane'])\n",
        "  predictions = model2.predict(test_data)\n",
        "  probability = predictions\n",
        "  predicted_class_index = np.argmax(probability)\n",
        "  print(predicted_class_index)\n",
        "  predicted_class = classes_labels[predicted_class_index]\n",
        "  print(predicted_class)\n",
        "  print(f'The owner of this song is : {predicted_class} ')\n",
        "  bot.send_message(chat_id= message.chat.id , text=f'The owner of this song is  :  {predicted_class}')\n",
        "\n",
        "\n",
        "bot.infinity_polling()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}